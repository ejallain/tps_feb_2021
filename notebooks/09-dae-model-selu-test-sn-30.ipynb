{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eria\\Anaconda3\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tps_feb_2021.config import config\n",
    "from tps_feb_2021.utils import add_noise, save_model, get_run_logdir, extract_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = config['data_dir']\n",
    "\n",
    "train_raw = pd.read_csv(data_dir + 'raw/train.csv')\n",
    "test_raw = pd.read_csv(data_dir + 'raw/test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_cols = [col for col in train_raw.columns if col[:4] == 'cont']\n",
    "cat_cols = [col for col in train_raw.columns if col[:3] == 'cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat0</th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>...</th>\n",
       "      <th>cont4</th>\n",
       "      <th>cont5</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>C</td>\n",
       "      <td>I</td>\n",
       "      <td>...</td>\n",
       "      <td>0.281421</td>\n",
       "      <td>0.881122</td>\n",
       "      <td>0.421650</td>\n",
       "      <td>0.741413</td>\n",
       "      <td>0.895799</td>\n",
       "      <td>0.802461</td>\n",
       "      <td>0.724417</td>\n",
       "      <td>0.701915</td>\n",
       "      <td>0.877618</td>\n",
       "      <td>0.719903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>A</td>\n",
       "      <td>F</td>\n",
       "      <td>...</td>\n",
       "      <td>0.282354</td>\n",
       "      <td>0.440011</td>\n",
       "      <td>0.346230</td>\n",
       "      <td>0.278495</td>\n",
       "      <td>0.593413</td>\n",
       "      <td>0.546056</td>\n",
       "      <td>0.613252</td>\n",
       "      <td>0.741289</td>\n",
       "      <td>0.326679</td>\n",
       "      <td>0.808464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>C</td>\n",
       "      <td>N</td>\n",
       "      <td>...</td>\n",
       "      <td>0.293756</td>\n",
       "      <td>0.914155</td>\n",
       "      <td>0.369602</td>\n",
       "      <td>0.832564</td>\n",
       "      <td>0.865620</td>\n",
       "      <td>0.825251</td>\n",
       "      <td>0.264104</td>\n",
       "      <td>0.695561</td>\n",
       "      <td>0.869133</td>\n",
       "      <td>0.828352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>D</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>G</td>\n",
       "      <td>K</td>\n",
       "      <td>...</td>\n",
       "      <td>0.769785</td>\n",
       "      <td>0.934138</td>\n",
       "      <td>0.578930</td>\n",
       "      <td>0.407313</td>\n",
       "      <td>0.868099</td>\n",
       "      <td>0.794402</td>\n",
       "      <td>0.494269</td>\n",
       "      <td>0.698125</td>\n",
       "      <td>0.809799</td>\n",
       "      <td>0.614766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>E</td>\n",
       "      <td>C</td>\n",
       "      <td>F</td>\n",
       "      <td>...</td>\n",
       "      <td>0.279105</td>\n",
       "      <td>0.382600</td>\n",
       "      <td>0.705940</td>\n",
       "      <td>0.325193</td>\n",
       "      <td>0.440967</td>\n",
       "      <td>0.462146</td>\n",
       "      <td>0.724447</td>\n",
       "      <td>0.683073</td>\n",
       "      <td>0.343457</td>\n",
       "      <td>0.297743</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  cat0 cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9  ...     cont4     cont5  \\\n",
       "0    A    B    A    A    B    D    A    E    C    I  ...  0.281421  0.881122   \n",
       "1    B    A    A    A    B    B    A    E    A    F  ...  0.282354  0.440011   \n",
       "2    A    A    A    C    B    D    A    B    C    N  ...  0.293756  0.914155   \n",
       "3    A    A    A    C    B    D    A    E    G    K  ...  0.769785  0.934138   \n",
       "4    A    B    A    A    B    B    A    E    C    F  ...  0.279105  0.382600   \n",
       "\n",
       "      cont6     cont7     cont8     cont9    cont10    cont11    cont12  \\\n",
       "0  0.421650  0.741413  0.895799  0.802461  0.724417  0.701915  0.877618   \n",
       "1  0.346230  0.278495  0.593413  0.546056  0.613252  0.741289  0.326679   \n",
       "2  0.369602  0.832564  0.865620  0.825251  0.264104  0.695561  0.869133   \n",
       "3  0.578930  0.407313  0.868099  0.794402  0.494269  0.698125  0.809799   \n",
       "4  0.705940  0.325193  0.440967  0.462146  0.724447  0.683073  0.343457   \n",
       "\n",
       "     cont13  \n",
       "0  0.719903  \n",
       "1  0.808464  \n",
       "2  0.828352  \n",
       "3  0.614766  \n",
       "4  0.297743  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.concat([train_raw[cat_cols + cont_cols], test_raw[cat_cols + cont_cols]])\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500000, 24)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_X = add_noise(X, p=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "cont_X_scaled = pd.DataFrame(scaler.fit_transform(X[cont_cols]), columns=cont_cols)\n",
    "cont_noisy_X_scaled = pd.DataFrame(scaler.fit_transform(noisy_X[cont_cols]), columns=cont_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_one_hot_cats = pd.get_dummies(noisy_X[cat_cols])\n",
    "one_hot_cats = pd.get_dummies(X[cat_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_X = pd.concat([noisy_one_hot_cats, cont_noisy_X_scaled], axis=1)\n",
    "X = pd.concat([one_hot_cats.reset_index(drop=True), cont_X_scaled.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500000, 70)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500000, 70)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cat0_A', 'cat0_B', 'cat1_A', 'cat1_B', 'cat2_A', 'cat2_B', 'cat3_A',\n",
       "       'cat3_B', 'cat3_C', 'cat3_D', 'cat4_A', 'cat4_B', 'cat4_C', 'cat4_D',\n",
       "       'cat5_A', 'cat5_B', 'cat5_C', 'cat5_D', 'cat6_A', 'cat6_B', 'cat6_C',\n",
       "       'cat6_D', 'cat6_E', 'cat6_G', 'cat6_H', 'cat6_I', 'cat7_A', 'cat7_B',\n",
       "       'cat7_C', 'cat7_D', 'cat7_E', 'cat7_F', 'cat7_G', 'cat7_I', 'cat8_A',\n",
       "       'cat8_B', 'cat8_C', 'cat8_D', 'cat8_E', 'cat8_F', 'cat8_G', 'cat9_A',\n",
       "       'cat9_B', 'cat9_C', 'cat9_D', 'cat9_E', 'cat9_F', 'cat9_G', 'cat9_H',\n",
       "       'cat9_I', 'cat9_J', 'cat9_K', 'cat9_L', 'cat9_M', 'cat9_N', 'cat9_O',\n",
       "       'cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7',\n",
       "       'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cat0_A', 'cat0_B', 'cat1_A', 'cat1_B', 'cat2_A', 'cat2_B', 'cat3_A',\n",
       "       'cat3_B', 'cat3_C', 'cat3_D', 'cat4_A', 'cat4_B', 'cat4_C', 'cat4_D',\n",
       "       'cat5_A', 'cat5_B', 'cat5_C', 'cat5_D', 'cat6_A', 'cat6_B', 'cat6_C',\n",
       "       'cat6_D', 'cat6_E', 'cat6_G', 'cat6_H', 'cat6_I', 'cat7_A', 'cat7_B',\n",
       "       'cat7_C', 'cat7_D', 'cat7_E', 'cat7_F', 'cat7_G', 'cat7_I', 'cat8_A',\n",
       "       'cat8_B', 'cat8_C', 'cat8_D', 'cat8_E', 'cat8_F', 'cat8_G', 'cat9_A',\n",
       "       'cat9_B', 'cat9_C', 'cat9_D', 'cat9_E', 'cat9_F', 'cat9_G', 'cat9_H',\n",
       "       'cat9_I', 'cat9_J', 'cat9_K', 'cat9_L', 'cat9_M', 'cat9_N', 'cat9_O',\n",
       "       'cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7',\n",
       "       'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Input(shape=noisy_X.shape[1:]),\n",
    "    keras.layers.Dense(500, activation='selu', kernel_initializer='lecun_normal'),\n",
    "    keras.layers.Dense(500, activation='selu', kernel_initializer='lecun_normal'),\n",
    "    keras.layers.Dense(500, activation='selu', kernel_initializer='lecun_normal'),\n",
    "    keras.layers.Dense(70)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.Adam(learning_rate=0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_logdir = get_run_logdir()\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2001\n",
      "15625/15625 [==============================] - 76s 5ms/step - loss: 0.1149\n",
      "Epoch 2/2001\n",
      "15625/15625 [==============================] - 69s 4ms/step - loss: 0.1079\n",
      "Epoch 3/2001\n",
      "15625/15625 [==============================] - 69s 4ms/step - loss: 0.1069\n",
      "Epoch 4/2001\n",
      "15625/15625 [==============================] - 69s 4ms/step - loss: 0.1064\n",
      "Epoch 5/2001\n",
      "15625/15625 [==============================] - 70s 4ms/step - loss: 0.1062\n",
      "Epoch 6/2001\n",
      "15625/15625 [==============================] - 68s 4ms/step - loss: 0.1060\n",
      "Epoch 7/2001\n",
      "15625/15625 [==============================] - 68s 4ms/step - loss: 0.1059\n",
      "Epoch 8/2001\n",
      "15625/15625 [==============================] - 69s 4ms/step - loss: 0.1057\n",
      "Epoch 9/2001\n",
      "15625/15625 [==============================] - 69s 4ms/step - loss: 0.1056\n",
      "Epoch 10/2001\n",
      "15625/15625 [==============================] - 71s 5ms/step - loss: 0.1056\n",
      "Epoch 11/2001\n",
      "15625/15625 [==============================] - 73s 5ms/step - loss: 0.1055\n",
      "Epoch 12/2001\n",
      "15625/15625 [==============================] - 73s 5ms/step - loss: 0.1055\n",
      "Epoch 13/2001\n",
      "15625/15625 [==============================] - 73s 5ms/step - loss: 0.1054\n",
      "Epoch 14/2001\n",
      "15625/15625 [==============================] - 73s 5ms/step - loss: 0.1054\n",
      "Epoch 15/2001\n",
      "15625/15625 [==============================] - 74s 5ms/step - loss: 0.1054\n",
      "Epoch 16/2001\n",
      "15625/15625 [==============================] - 73s 5ms/step - loss: 0.1053\n",
      "Epoch 17/2001\n",
      "15625/15625 [==============================] - 72s 5ms/step - loss: 0.1053\n",
      "Epoch 18/2001\n",
      "15625/15625 [==============================] - 73s 5ms/step - loss: 0.1053\n",
      "Epoch 19/2001\n",
      "15625/15625 [==============================] - 73s 5ms/step - loss: 0.1053\n",
      "Epoch 20/2001\n",
      "15625/15625 [==============================] - 73s 5ms/step - loss: 0.1053\n",
      "Epoch 21/2001\n",
      "15625/15625 [==============================] - 74s 5ms/step - loss: 0.1053\n",
      "Epoch 22/2001\n",
      "15625/15625 [==============================] - 73s 5ms/step - loss: 0.1052\n",
      "Epoch 23/2001\n",
      "15625/15625 [==============================] - 74s 5ms/step - loss: 0.1052\n",
      "Epoch 24/2001\n",
      "15625/15625 [==============================] - 74s 5ms/step - loss: 0.1052\n",
      "Epoch 25/2001\n",
      "15625/15625 [==============================] - 75s 5ms/step - loss: 0.1052\n",
      "Epoch 26/2001\n",
      "15625/15625 [==============================] - 74s 5ms/step - loss: 0.1052\n",
      "Epoch 27/2001\n",
      "15625/15625 [==============================] - 74s 5ms/step - loss: 0.1052\n",
      "Epoch 28/2001\n",
      "15625/15625 [==============================] - 76s 5ms/step - loss: 0.1052\n",
      "Epoch 29/2001\n",
      "15625/15625 [==============================] - 75s 5ms/step - loss: 0.1051\n",
      "Epoch 30/2001\n",
      "15625/15625 [==============================] - 75s 5ms/step - loss: 0.1051\n",
      "Epoch 31/2001\n",
      "15625/15625 [==============================] - 75s 5ms/step - loss: 0.1051\n",
      "Epoch 32/2001\n",
      "15625/15625 [==============================] - 76s 5ms/step - loss: 0.1051\n",
      "Epoch 33/2001\n",
      "15625/15625 [==============================] - 76s 5ms/step - loss: 0.1051\n",
      "Epoch 34/2001\n",
      "15625/15625 [==============================] - 76s 5ms/step - loss: 0.1051\n",
      "Epoch 35/2001\n",
      "15625/15625 [==============================] - 76s 5ms/step - loss: 0.1051\n",
      "Epoch 36/2001\n",
      "15625/15625 [==============================] - 77s 5ms/step - loss: 0.1051\n",
      "Epoch 37/2001\n",
      "15625/15625 [==============================] - 77s 5ms/step - loss: 0.1051\n",
      "Epoch 38/2001\n",
      "15625/15625 [==============================] - 78s 5ms/step - loss: 0.1051\n",
      "Epoch 39/2001\n",
      "15625/15625 [==============================] - 78s 5ms/step - loss: 0.1051\n",
      "Epoch 40/2001\n",
      "15625/15625 [==============================] - 79s 5ms/step - loss: 0.1051\n",
      "Epoch 41/2001\n",
      "15625/15625 [==============================] - 79s 5ms/step - loss: 0.1051\n",
      "Epoch 42/2001\n",
      "15625/15625 [==============================] - 78s 5ms/step - loss: 0.1051\n",
      "Epoch 43/2001\n",
      "15625/15625 [==============================] - 79s 5ms/step - loss: 0.1051\n",
      "Epoch 44/2001\n",
      "15625/15625 [==============================] - 79s 5ms/step - loss: 0.1051\n",
      "Epoch 45/2001\n",
      "15625/15625 [==============================] - 80s 5ms/step - loss: 0.1051\n",
      "Epoch 46/2001\n",
      "15625/15625 [==============================] - 79s 5ms/step - loss: 0.1051\n",
      "Epoch 47/2001\n",
      "15625/15625 [==============================] - 79s 5ms/step - loss: 0.1051\n",
      "Epoch 48/2001\n",
      "15625/15625 [==============================] - 80s 5ms/step - loss: 0.1051\n",
      "Epoch 49/2001\n",
      "15625/15625 [==============================] - 82s 5ms/step - loss: 0.1051\n",
      "Epoch 50/2001\n",
      "15625/15625 [==============================] - 82s 5ms/step - loss: 0.1051\n",
      "Epoch 51/2001\n",
      "15625/15625 [==============================] - 82s 5ms/step - loss: 0.1051\n",
      "Epoch 52/2001\n",
      "15625/15625 [==============================] - 82s 5ms/step - loss: 0.1051\n",
      "Epoch 53/2001\n",
      "15625/15625 [==============================] - 83s 5ms/step - loss: 0.1051\n",
      "Epoch 54/2001\n",
      "15625/15625 [==============================] - 83s 5ms/step - loss: 0.1051\n",
      "Epoch 55/2001\n",
      "15625/15625 [==============================] - 83s 5ms/step - loss: 0.1051\n",
      "Epoch 56/2001\n",
      "15625/15625 [==============================] - 83s 5ms/step - loss: 0.1050\n",
      "Epoch 57/2001\n",
      "15625/15625 [==============================] - 83s 5ms/step - loss: 0.1051\n",
      "Epoch 58/2001\n",
      "15625/15625 [==============================] - 84s 5ms/step - loss: 0.1051\n",
      "Epoch 59/2001\n",
      "15625/15625 [==============================] - 85s 5ms/step - loss: 0.1050\n",
      "Epoch 60/2001\n",
      "15625/15625 [==============================] - 85s 5ms/step - loss: 0.1050\n",
      "Epoch 61/2001\n",
      "15625/15625 [==============================] - 86s 5ms/step - loss: 0.1050\n",
      "Epoch 62/2001\n",
      "15625/15625 [==============================] - 86s 5ms/step - loss: 0.1050\n",
      "Epoch 63/2001\n",
      "15625/15625 [==============================] - 87s 6ms/step - loss: 0.1050\n",
      "Epoch 64/2001\n",
      "15625/15625 [==============================] - 87s 6ms/step - loss: 0.1050\n",
      "Epoch 65/2001\n",
      "15625/15625 [==============================] - 88s 6ms/step - loss: 0.1050\n",
      "Epoch 66/2001\n",
      "15625/15625 [==============================] - 88s 6ms/step - loss: 0.1050\n",
      "Epoch 67/2001\n",
      "15625/15625 [==============================] - 89s 6ms/step - loss: 0.1050\n",
      "Epoch 68/2001\n",
      "15625/15625 [==============================] - 89s 6ms/step - loss: 0.1050\n",
      "Epoch 69/2001\n",
      "15625/15625 [==============================] - 90s 6ms/step - loss: 0.1050\n",
      "Epoch 70/2001\n",
      "15625/15625 [==============================] - 90s 6ms/step - loss: 0.1050\n",
      "Epoch 71/2001\n",
      "15625/15625 [==============================] - 90s 6ms/step - loss: 0.1050\n",
      "Epoch 72/2001\n",
      "15625/15625 [==============================] - 91s 6ms/step - loss: 0.1050\n",
      "Epoch 73/2001\n",
      "15625/15625 [==============================] - 91s 6ms/step - loss: 0.1050\n",
      "Epoch 74/2001\n",
      "15625/15625 [==============================] - 93s 6ms/step - loss: 0.1050\n",
      "Epoch 75/2001\n",
      "15625/15625 [==============================] - 93s 6ms/step - loss: 0.1050\n",
      "Epoch 76/2001\n",
      "15625/15625 [==============================] - 95s 6ms/step - loss: 0.1050\n",
      "Epoch 77/2001\n",
      "15625/15625 [==============================] - 95s 6ms/step - loss: 0.1050\n",
      "Epoch 78/2001\n",
      "15625/15625 [==============================] - 95s 6ms/step - loss: 0.1050\n",
      "Epoch 79/2001\n",
      "15625/15625 [==============================] - 95s 6ms/step - loss: 0.1050\n",
      "Epoch 80/2001\n",
      "15625/15625 [==============================] - 95s 6ms/step - loss: 0.1050\n",
      "Epoch 81/2001\n",
      "15625/15625 [==============================] - 114s 7ms/step - loss: 0.1050\n",
      "Epoch 82/2001\n",
      "15625/15625 [==============================] - 137s 9ms/step - loss: 0.1050\n",
      "Epoch 83/2001\n",
      "15625/15625 [==============================] - 138s 9ms/step - loss: 0.1050\n",
      "Epoch 84/2001\n",
      "15625/15625 [==============================] - 139s 9ms/step - loss: 0.1050\n",
      "Epoch 85/2001\n",
      "15625/15625 [==============================] - 140s 9ms/step - loss: 0.1050\n",
      "Epoch 86/2001\n",
      "15625/15625 [==============================] - 139s 9ms/step - loss: 0.1050\n",
      "Epoch 87/2001\n",
      "15625/15625 [==============================] - 141s 9ms/step - loss: 0.1050\n",
      "Epoch 88/2001\n",
      "15625/15625 [==============================] - 142s 9ms/step - loss: 0.1050\n",
      "Epoch 89/2001\n",
      "15625/15625 [==============================] - 144s 9ms/step - loss: 0.1050\n",
      "Epoch 90/2001\n",
      "15625/15625 [==============================] - 144s 9ms/step - loss: 0.1050\n",
      "Epoch 91/2001\n",
      "15625/15625 [==============================] - 149s 10ms/step - loss: 0.1050\n",
      "Epoch 92/2001\n",
      "15625/15625 [==============================] - 146s 9ms/step - loss: 0.1050\n",
      "Epoch 93/2001\n",
      "15625/15625 [==============================] - 148s 9ms/step - loss: 0.1050\n",
      "Epoch 94/2001\n",
      "15625/15625 [==============================] - 148s 9ms/step - loss: 0.1050\n",
      "Epoch 95/2001\n",
      "15625/15625 [==============================] - 150s 10ms/step - loss: 0.1050\n",
      "Epoch 96/2001\n",
      "15625/15625 [==============================] - 150s 10ms/step - loss: 0.1050\n",
      "Epoch 97/2001\n",
      "15625/15625 [==============================] - 150s 10ms/step - loss: 0.1050\n",
      "Epoch 98/2001\n",
      "15625/15625 [==============================] - 152s 10ms/step - loss: 0.1050\n",
      "Epoch 99/2001\n",
      "15625/15625 [==============================] - 151s 10ms/step - loss: 0.1050\n",
      "Epoch 100/2001\n",
      "15625/15625 [==============================] - 138s 9ms/step - loss: 0.1050\n",
      "Epoch 101/2001\n",
      "15625/15625 [==============================] - 156s 10ms/step - loss: 0.1050\n",
      "Epoch 102/2001\n",
      "15625/15625 [==============================] - 156s 10ms/step - loss: 0.1050\n",
      "Epoch 103/2001\n",
      "15625/15625 [==============================] - 154s 10ms/step - loss: 0.1050\n",
      "Epoch 104/2001\n",
      "15625/15625 [==============================] - 148s 9ms/step - loss: 0.1050\n",
      "Epoch 105/2001\n",
      "15625/15625 [==============================] - 159s 10ms/step - loss: 0.1050\n",
      "Epoch 106/2001\n",
      "15625/15625 [==============================] - 153s 10ms/step - loss: 0.1050\n",
      "Epoch 107/2001\n",
      "15625/15625 [==============================] - 158s 10ms/step - loss: 0.1050\n",
      "Epoch 108/2001\n",
      "15625/15625 [==============================] - 162s 10ms/step - loss: 0.1050\n",
      "Epoch 109/2001\n",
      "15625/15625 [==============================] - 152s 10ms/step - loss: 0.1050\n",
      "Epoch 110/2001\n",
      "15625/15625 [==============================] - 151s 10ms/step - loss: 0.1050\n",
      "Epoch 111/2001\n",
      "15625/15625 [==============================] - 162s 10ms/step - loss: 0.1050\n",
      "Epoch 112/2001\n",
      "15625/15625 [==============================] - 172s 11ms/step - loss: 0.1050\n",
      "Epoch 113/2001\n",
      "15625/15625 [==============================] - 171s 11ms/step - loss: 0.1050\n",
      "Epoch 114/2001\n",
      "15625/15625 [==============================] - 172s 11ms/step - loss: 0.1050\n",
      "Epoch 115/2001\n",
      "15625/15625 [==============================] - 171s 11ms/step - loss: 0.1050\n",
      "Epoch 116/2001\n",
      "15625/15625 [==============================] - 159s 10ms/step - loss: 0.1050\n",
      "Epoch 117/2001\n",
      "15625/15625 [==============================] - 171s 11ms/step - loss: 0.1050\n",
      "Epoch 118/2001\n",
      "15625/15625 [==============================] - 173s 11ms/step - loss: 0.1050\n",
      "Epoch 119/2001\n",
      "15625/15625 [==============================] - 171s 11ms/step - loss: 0.1050\n",
      "Epoch 120/2001\n",
      "15625/15625 [==============================] - 174s 11ms/step - loss: 0.1050\n",
      "Epoch 121/2001\n",
      "15625/15625 [==============================] - 175s 11ms/step - loss: 0.1050\n",
      "Epoch 122/2001\n",
      "15625/15625 [==============================] - 175s 11ms/step - loss: 0.1050\n",
      "Epoch 123/2001\n",
      "15625/15625 [==============================] - 175s 11ms/step - loss: 0.1050\n",
      "Epoch 124/2001\n",
      "15625/15625 [==============================] - 176s 11ms/step - loss: 0.1050\n",
      "Epoch 125/2001\n",
      "15625/15625 [==============================] - 175s 11ms/step - loss: 0.1050\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(noisy_X, X, epochs=2001,\n",
    "                    callbacks=[tensorboard_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, '06_dae_model_selu_sn_30.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = keras.models.load_model('../models/06_dae_model_swap_noise_30.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasTensor: shape=(None, 70) dtype=float32 (created by layer 'input_1')>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15625/15625 [==============================] - 93s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "features_df = extract_features(model, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1490</th>\n",
       "      <th>1491</th>\n",
       "      <th>1492</th>\n",
       "      <th>1493</th>\n",
       "      <th>1494</th>\n",
       "      <th>1495</th>\n",
       "      <th>1496</th>\n",
       "      <th>1497</th>\n",
       "      <th>1498</th>\n",
       "      <th>1499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.058319</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>0.052489</td>\n",
       "      <td>0.021048</td>\n",
       "      <td>-0.794594</td>\n",
       "      <td>-0.004756</td>\n",
       "      <td>0.010086</td>\n",
       "      <td>-0.715352</td>\n",
       "      <td>0.029382</td>\n",
       "      <td>0.014650</td>\n",
       "      <td>...</td>\n",
       "      <td>0.065788</td>\n",
       "      <td>-0.086147</td>\n",
       "      <td>0.069789</td>\n",
       "      <td>0.003956</td>\n",
       "      <td>0.105133</td>\n",
       "      <td>-0.048815</td>\n",
       "      <td>0.129096</td>\n",
       "      <td>0.064421</td>\n",
       "      <td>0.007491</td>\n",
       "      <td>0.002482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.040359</td>\n",
       "      <td>0.082081</td>\n",
       "      <td>0.037943</td>\n",
       "      <td>0.010939</td>\n",
       "      <td>-1.328174</td>\n",
       "      <td>-0.033000</td>\n",
       "      <td>0.038045</td>\n",
       "      <td>1.619082</td>\n",
       "      <td>0.050926</td>\n",
       "      <td>0.001353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007663</td>\n",
       "      <td>0.007641</td>\n",
       "      <td>0.068315</td>\n",
       "      <td>0.003265</td>\n",
       "      <td>-0.161186</td>\n",
       "      <td>0.164613</td>\n",
       "      <td>0.052404</td>\n",
       "      <td>0.121457</td>\n",
       "      <td>0.091269</td>\n",
       "      <td>0.002464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.108792</td>\n",
       "      <td>0.053552</td>\n",
       "      <td>0.019014</td>\n",
       "      <td>-0.110000</td>\n",
       "      <td>-0.339576</td>\n",
       "      <td>-0.141808</td>\n",
       "      <td>0.031946</td>\n",
       "      <td>1.223386</td>\n",
       "      <td>0.072953</td>\n",
       "      <td>0.066531</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046698</td>\n",
       "      <td>0.092065</td>\n",
       "      <td>0.092808</td>\n",
       "      <td>0.037581</td>\n",
       "      <td>0.216625</td>\n",
       "      <td>0.119879</td>\n",
       "      <td>0.064635</td>\n",
       "      <td>0.078915</td>\n",
       "      <td>0.081021</td>\n",
       "      <td>0.142035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.029734</td>\n",
       "      <td>0.062232</td>\n",
       "      <td>-0.012232</td>\n",
       "      <td>-0.053900</td>\n",
       "      <td>-0.932599</td>\n",
       "      <td>-0.062278</td>\n",
       "      <td>0.019316</td>\n",
       "      <td>1.155396</td>\n",
       "      <td>0.070561</td>\n",
       "      <td>0.039300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032138</td>\n",
       "      <td>0.020613</td>\n",
       "      <td>0.038386</td>\n",
       "      <td>-0.001263</td>\n",
       "      <td>0.070591</td>\n",
       "      <td>0.048093</td>\n",
       "      <td>0.007249</td>\n",
       "      <td>-0.016039</td>\n",
       "      <td>0.040505</td>\n",
       "      <td>0.070142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.019453</td>\n",
       "      <td>0.065288</td>\n",
       "      <td>0.033731</td>\n",
       "      <td>0.031940</td>\n",
       "      <td>-1.119124</td>\n",
       "      <td>0.023137</td>\n",
       "      <td>0.031918</td>\n",
       "      <td>-0.020218</td>\n",
       "      <td>0.015763</td>\n",
       "      <td>0.015059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028058</td>\n",
       "      <td>0.028873</td>\n",
       "      <td>0.093481</td>\n",
       "      <td>0.001618</td>\n",
       "      <td>-0.212316</td>\n",
       "      <td>0.085945</td>\n",
       "      <td>0.018746</td>\n",
       "      <td>0.127637</td>\n",
       "      <td>0.031417</td>\n",
       "      <td>-0.108234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499995</th>\n",
       "      <td>0.066229</td>\n",
       "      <td>0.045676</td>\n",
       "      <td>0.003087</td>\n",
       "      <td>0.060369</td>\n",
       "      <td>-0.754635</td>\n",
       "      <td>-0.047248</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>-0.597162</td>\n",
       "      <td>0.071692</td>\n",
       "      <td>0.034272</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089646</td>\n",
       "      <td>-0.087097</td>\n",
       "      <td>0.030235</td>\n",
       "      <td>-0.012128</td>\n",
       "      <td>0.128132</td>\n",
       "      <td>-0.070481</td>\n",
       "      <td>0.058289</td>\n",
       "      <td>0.032597</td>\n",
       "      <td>0.035057</td>\n",
       "      <td>0.071568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499996</th>\n",
       "      <td>0.005686</td>\n",
       "      <td>0.094318</td>\n",
       "      <td>-0.011155</td>\n",
       "      <td>0.064207</td>\n",
       "      <td>1.151271</td>\n",
       "      <td>-0.038018</td>\n",
       "      <td>0.090155</td>\n",
       "      <td>0.119251</td>\n",
       "      <td>0.067192</td>\n",
       "      <td>0.040168</td>\n",
       "      <td>...</td>\n",
       "      <td>0.057818</td>\n",
       "      <td>0.057854</td>\n",
       "      <td>0.120594</td>\n",
       "      <td>-0.022143</td>\n",
       "      <td>-0.004959</td>\n",
       "      <td>0.034844</td>\n",
       "      <td>0.019324</td>\n",
       "      <td>-0.029615</td>\n",
       "      <td>0.013295</td>\n",
       "      <td>0.053215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499997</th>\n",
       "      <td>0.013255</td>\n",
       "      <td>0.066388</td>\n",
       "      <td>0.010527</td>\n",
       "      <td>0.012821</td>\n",
       "      <td>0.379246</td>\n",
       "      <td>0.016940</td>\n",
       "      <td>0.031929</td>\n",
       "      <td>0.130853</td>\n",
       "      <td>0.064136</td>\n",
       "      <td>0.026497</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031966</td>\n",
       "      <td>0.051732</td>\n",
       "      <td>0.097315</td>\n",
       "      <td>0.013994</td>\n",
       "      <td>0.011130</td>\n",
       "      <td>0.048667</td>\n",
       "      <td>0.030291</td>\n",
       "      <td>0.017709</td>\n",
       "      <td>0.019820</td>\n",
       "      <td>-0.049556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499998</th>\n",
       "      <td>0.038217</td>\n",
       "      <td>0.001306</td>\n",
       "      <td>0.017454</td>\n",
       "      <td>-0.049669</td>\n",
       "      <td>0.017816</td>\n",
       "      <td>-0.099921</td>\n",
       "      <td>0.018635</td>\n",
       "      <td>1.418504</td>\n",
       "      <td>0.036142</td>\n",
       "      <td>-0.008843</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024920</td>\n",
       "      <td>-0.073589</td>\n",
       "      <td>0.089761</td>\n",
       "      <td>-0.028511</td>\n",
       "      <td>0.053014</td>\n",
       "      <td>0.065833</td>\n",
       "      <td>0.144459</td>\n",
       "      <td>0.039169</td>\n",
       "      <td>0.030655</td>\n",
       "      <td>0.022318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499999</th>\n",
       "      <td>0.032696</td>\n",
       "      <td>0.026971</td>\n",
       "      <td>-0.018408</td>\n",
       "      <td>-0.099344</td>\n",
       "      <td>-0.785696</td>\n",
       "      <td>-0.074921</td>\n",
       "      <td>-0.008523</td>\n",
       "      <td>1.285742</td>\n",
       "      <td>0.024531</td>\n",
       "      <td>-0.080622</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068833</td>\n",
       "      <td>0.013693</td>\n",
       "      <td>-0.078733</td>\n",
       "      <td>-0.002431</td>\n",
       "      <td>0.052128</td>\n",
       "      <td>0.089253</td>\n",
       "      <td>0.057797</td>\n",
       "      <td>-0.081992</td>\n",
       "      <td>0.051843</td>\n",
       "      <td>0.056879</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500000 rows × 1500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6     \\\n",
       "0       0.058319  0.000465  0.052489  0.021048 -0.794594 -0.004756  0.010086   \n",
       "1       0.040359  0.082081  0.037943  0.010939 -1.328174 -0.033000  0.038045   \n",
       "2       0.108792  0.053552  0.019014 -0.110000 -0.339576 -0.141808  0.031946   \n",
       "3       0.029734  0.062232 -0.012232 -0.053900 -0.932599 -0.062278  0.019316   \n",
       "4       0.019453  0.065288  0.033731  0.031940 -1.119124  0.023137  0.031918   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "499995  0.066229  0.045676  0.003087  0.060369 -0.754635 -0.047248  0.000370   \n",
       "499996  0.005686  0.094318 -0.011155  0.064207  1.151271 -0.038018  0.090155   \n",
       "499997  0.013255  0.066388  0.010527  0.012821  0.379246  0.016940  0.031929   \n",
       "499998  0.038217  0.001306  0.017454 -0.049669  0.017816 -0.099921  0.018635   \n",
       "499999  0.032696  0.026971 -0.018408 -0.099344 -0.785696 -0.074921 -0.008523   \n",
       "\n",
       "            7         8         9     ...      1490      1491      1492  \\\n",
       "0      -0.715352  0.029382  0.014650  ...  0.065788 -0.086147  0.069789   \n",
       "1       1.619082  0.050926  0.001353  ...  0.007663  0.007641  0.068315   \n",
       "2       1.223386  0.072953  0.066531  ...  0.046698  0.092065  0.092808   \n",
       "3       1.155396  0.070561  0.039300  ...  0.032138  0.020613  0.038386   \n",
       "4      -0.020218  0.015763  0.015059  ...  0.028058  0.028873  0.093481   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "499995 -0.597162  0.071692  0.034272  ...  0.089646 -0.087097  0.030235   \n",
       "499996  0.119251  0.067192  0.040168  ...  0.057818  0.057854  0.120594   \n",
       "499997  0.130853  0.064136  0.026497  ...  0.031966  0.051732  0.097315   \n",
       "499998  1.418504  0.036142 -0.008843  ...  0.024920 -0.073589  0.089761   \n",
       "499999  1.285742  0.024531 -0.080622  ...  0.068833  0.013693 -0.078733   \n",
       "\n",
       "            1493      1494      1495      1496      1497      1498      1499  \n",
       "0       0.003956  0.105133 -0.048815  0.129096  0.064421  0.007491  0.002482  \n",
       "1       0.003265 -0.161186  0.164613  0.052404  0.121457  0.091269  0.002464  \n",
       "2       0.037581  0.216625  0.119879  0.064635  0.078915  0.081021  0.142035  \n",
       "3      -0.001263  0.070591  0.048093  0.007249 -0.016039  0.040505  0.070142  \n",
       "4       0.001618 -0.212316  0.085945  0.018746  0.127637  0.031417 -0.108234  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "499995 -0.012128  0.128132 -0.070481  0.058289  0.032597  0.035057  0.071568  \n",
       "499996 -0.022143 -0.004959  0.034844  0.019324 -0.029615  0.013295  0.053215  \n",
       "499997  0.013994  0.011130  0.048667  0.030291  0.017709  0.019820 -0.049556  \n",
       "499998 -0.028511  0.053014  0.065833  0.144459  0.039169  0.030655  0.022318  \n",
       "499999 -0.002431  0.052128  0.089253  0.057797 -0.081992  0.051843  0.056879  \n",
       "\n",
       "[500000 rows x 1500 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>1490</th>\n",
       "      <th>1491</th>\n",
       "      <th>1492</th>\n",
       "      <th>1493</th>\n",
       "      <th>1494</th>\n",
       "      <th>1495</th>\n",
       "      <th>1496</th>\n",
       "      <th>1497</th>\n",
       "      <th>1498</th>\n",
       "      <th>1499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>500000.000000</td>\n",
       "      <td>500000.000000</td>\n",
       "      <td>500000.000000</td>\n",
       "      <td>500000.000000</td>\n",
       "      <td>500000.000000</td>\n",
       "      <td>500000.000000</td>\n",
       "      <td>500000.000000</td>\n",
       "      <td>500000.000000</td>\n",
       "      <td>500000.000000</td>\n",
       "      <td>500000.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>500000.000000</td>\n",
       "      <td>500000.000000</td>\n",
       "      <td>500000.000000</td>\n",
       "      <td>500000.000000</td>\n",
       "      <td>500000.000000</td>\n",
       "      <td>500000.000000</td>\n",
       "      <td>500000.000000</td>\n",
       "      <td>500000.000000</td>\n",
       "      <td>500000.000000</td>\n",
       "      <td>500000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.025102</td>\n",
       "      <td>0.059247</td>\n",
       "      <td>0.007537</td>\n",
       "      <td>0.013929</td>\n",
       "      <td>-0.440030</td>\n",
       "      <td>-0.027438</td>\n",
       "      <td>0.040679</td>\n",
       "      <td>0.269557</td>\n",
       "      <td>0.042120</td>\n",
       "      <td>0.021308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055828</td>\n",
       "      <td>0.028474</td>\n",
       "      <td>0.073996</td>\n",
       "      <td>-0.009258</td>\n",
       "      <td>-0.020578</td>\n",
       "      <td>0.059913</td>\n",
       "      <td>0.040509</td>\n",
       "      <td>0.041072</td>\n",
       "      <td>0.040587</td>\n",
       "      <td>0.006793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.024557</td>\n",
       "      <td>0.029455</td>\n",
       "      <td>0.035191</td>\n",
       "      <td>0.040105</td>\n",
       "      <td>0.721729</td>\n",
       "      <td>0.049846</td>\n",
       "      <td>0.028351</td>\n",
       "      <td>0.805048</td>\n",
       "      <td>0.019014</td>\n",
       "      <td>0.036980</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053463</td>\n",
       "      <td>0.048431</td>\n",
       "      <td>0.097760</td>\n",
       "      <td>0.059845</td>\n",
       "      <td>0.079519</td>\n",
       "      <td>0.052592</td>\n",
       "      <td>0.046877</td>\n",
       "      <td>0.057809</td>\n",
       "      <td>0.034782</td>\n",
       "      <td>0.075069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.390449</td>\n",
       "      <td>-0.367570</td>\n",
       "      <td>-0.665650</td>\n",
       "      <td>-0.305884</td>\n",
       "      <td>-1.734650</td>\n",
       "      <td>-0.465273</td>\n",
       "      <td>-0.455463</td>\n",
       "      <td>-1.023035</td>\n",
       "      <td>-0.210573</td>\n",
       "      <td>-0.340600</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.361819</td>\n",
       "      <td>-0.288986</td>\n",
       "      <td>-1.332452</td>\n",
       "      <td>-0.397591</td>\n",
       "      <td>-0.410056</td>\n",
       "      <td>-0.312431</td>\n",
       "      <td>-0.267537</td>\n",
       "      <td>-0.383548</td>\n",
       "      <td>-0.258766</td>\n",
       "      <td>-0.288700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.012979</td>\n",
       "      <td>0.040253</td>\n",
       "      <td>-0.011707</td>\n",
       "      <td>-0.007401</td>\n",
       "      <td>-1.014889</td>\n",
       "      <td>-0.063687</td>\n",
       "      <td>0.024501</td>\n",
       "      <td>-0.521208</td>\n",
       "      <td>0.030642</td>\n",
       "      <td>0.002702</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024861</td>\n",
       "      <td>0.006043</td>\n",
       "      <td>0.045827</td>\n",
       "      <td>-0.046013</td>\n",
       "      <td>-0.072785</td>\n",
       "      <td>0.028232</td>\n",
       "      <td>0.014013</td>\n",
       "      <td>0.010669</td>\n",
       "      <td>0.020831</td>\n",
       "      <td>-0.043727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.026807</td>\n",
       "      <td>0.060236</td>\n",
       "      <td>0.012341</td>\n",
       "      <td>0.017576</td>\n",
       "      <td>-0.578595</td>\n",
       "      <td>-0.019833</td>\n",
       "      <td>0.042631</td>\n",
       "      <td>0.161118</td>\n",
       "      <td>0.042805</td>\n",
       "      <td>0.023326</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056898</td>\n",
       "      <td>0.034714</td>\n",
       "      <td>0.081152</td>\n",
       "      <td>0.003235</td>\n",
       "      <td>-0.007854</td>\n",
       "      <td>0.058954</td>\n",
       "      <td>0.041800</td>\n",
       "      <td>0.046626</td>\n",
       "      <td>0.044110</td>\n",
       "      <td>0.011554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.040050</td>\n",
       "      <td>0.078863</td>\n",
       "      <td>0.031213</td>\n",
       "      <td>0.039777</td>\n",
       "      <td>0.078632</td>\n",
       "      <td>0.012389</td>\n",
       "      <td>0.059547</td>\n",
       "      <td>1.102319</td>\n",
       "      <td>0.054550</td>\n",
       "      <td>0.044640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089882</td>\n",
       "      <td>0.060162</td>\n",
       "      <td>0.114661</td>\n",
       "      <td>0.031445</td>\n",
       "      <td>0.034587</td>\n",
       "      <td>0.092632</td>\n",
       "      <td>0.070326</td>\n",
       "      <td>0.081217</td>\n",
       "      <td>0.064362</td>\n",
       "      <td>0.054233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.412326</td>\n",
       "      <td>0.314849</td>\n",
       "      <td>0.310028</td>\n",
       "      <td>0.336443</td>\n",
       "      <td>3.080727</td>\n",
       "      <td>0.331823</td>\n",
       "      <td>0.394082</td>\n",
       "      <td>2.545892</td>\n",
       "      <td>0.346485</td>\n",
       "      <td>0.392960</td>\n",
       "      <td>...</td>\n",
       "      <td>0.327842</td>\n",
       "      <td>0.213785</td>\n",
       "      <td>0.637291</td>\n",
       "      <td>0.209276</td>\n",
       "      <td>0.285022</td>\n",
       "      <td>0.355897</td>\n",
       "      <td>0.275079</td>\n",
       "      <td>0.275906</td>\n",
       "      <td>0.209911</td>\n",
       "      <td>0.369965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 1500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0              1              2              3     \\\n",
       "count  500000.000000  500000.000000  500000.000000  500000.000000   \n",
       "mean        0.025102       0.059247       0.007537       0.013929   \n",
       "std         0.024557       0.029455       0.035191       0.040105   \n",
       "min        -0.390449      -0.367570      -0.665650      -0.305884   \n",
       "25%         0.012979       0.040253      -0.011707      -0.007401   \n",
       "50%         0.026807       0.060236       0.012341       0.017576   \n",
       "75%         0.040050       0.078863       0.031213       0.039777   \n",
       "max         0.412326       0.314849       0.310028       0.336443   \n",
       "\n",
       "                4              5              6              7     \\\n",
       "count  500000.000000  500000.000000  500000.000000  500000.000000   \n",
       "mean       -0.440030      -0.027438       0.040679       0.269557   \n",
       "std         0.721729       0.049846       0.028351       0.805048   \n",
       "min        -1.734650      -0.465273      -0.455463      -1.023035   \n",
       "25%        -1.014889      -0.063687       0.024501      -0.521208   \n",
       "50%        -0.578595      -0.019833       0.042631       0.161118   \n",
       "75%         0.078632       0.012389       0.059547       1.102319   \n",
       "max         3.080727       0.331823       0.394082       2.545892   \n",
       "\n",
       "                8              9     ...           1490           1491  \\\n",
       "count  500000.000000  500000.000000  ...  500000.000000  500000.000000   \n",
       "mean        0.042120       0.021308  ...       0.055828       0.028474   \n",
       "std         0.019014       0.036980  ...       0.053463       0.048431   \n",
       "min        -0.210573      -0.340600  ...      -0.361819      -0.288986   \n",
       "25%         0.030642       0.002702  ...       0.024861       0.006043   \n",
       "50%         0.042805       0.023326  ...       0.056898       0.034714   \n",
       "75%         0.054550       0.044640  ...       0.089882       0.060162   \n",
       "max         0.346485       0.392960  ...       0.327842       0.213785   \n",
       "\n",
       "                1492           1493           1494           1495  \\\n",
       "count  500000.000000  500000.000000  500000.000000  500000.000000   \n",
       "mean        0.073996      -0.009258      -0.020578       0.059913   \n",
       "std         0.097760       0.059845       0.079519       0.052592   \n",
       "min        -1.332452      -0.397591      -0.410056      -0.312431   \n",
       "25%         0.045827      -0.046013      -0.072785       0.028232   \n",
       "50%         0.081152       0.003235      -0.007854       0.058954   \n",
       "75%         0.114661       0.031445       0.034587       0.092632   \n",
       "max         0.637291       0.209276       0.285022       0.355897   \n",
       "\n",
       "                1496           1497           1498           1499  \n",
       "count  500000.000000  500000.000000  500000.000000  500000.000000  \n",
       "mean        0.040509       0.041072       0.040587       0.006793  \n",
       "std         0.046877       0.057809       0.034782       0.075069  \n",
       "min        -0.267537      -0.383548      -0.258766      -0.288700  \n",
       "25%         0.014013       0.010669       0.020831      -0.043727  \n",
       "50%         0.041800       0.046626       0.044110       0.011554  \n",
       "75%         0.070326       0.081217       0.064362       0.054233  \n",
       "max         0.275079       0.275906       0.209911       0.369965  \n",
       "\n",
       "[8 rows x 1500 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "descrip = features_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05924705043435097"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descrip.loc['mean'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "active features =  1500\n",
      "unused features =  0\n"
     ]
    }
   ],
   "source": [
    "unused_features = []\n",
    "active_features = []\n",
    "for col in features_df.columns:\n",
    "    if descrip.loc['mean'][col] == 0.0:\n",
    "        unused_features.append(col)\n",
    "    else:\n",
    "        active_features.append(col)\n",
    "print('active features = ', len(active_features))\n",
    "print('unused features = ', len(unused_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_features_df = features_df[active_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_features_df = active_features_df[:len(train_raw)]\n",
    "len(X_train_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_features_df = active_features_df[len(train_raw):]\n",
    "len(X_test_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_features_df.to_csv(data_dir + 'processed/X_train_dae_encoded_selu_sn_30.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_features_df.to_csv(data_dir + 'processed/X_test_dae_encoded_selu_sn_30.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test linear model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_csv(data_dir + 'processed/y_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eria\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_ridge.py:157: LinAlgWarning: Ill-conditioned matrix (rcond=4.99065e-09): result may not be accurate.\n",
      "  return linalg.solve(A, Xy, sym_pos=True, overwrite_a=True).T\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1, solver='cholesky')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ridge_reg = Ridge(alpha=1, solver='cholesky')\n",
    "ridge_reg.fit(X_train_features_df, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ridge_reg.predict(X_train_features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8548836979129444"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y, y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2acbe491751959775267b317a38ea4b53e5da97b39b9edb9aff51f85edb8ae0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
